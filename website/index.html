
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Starter Template for Bootstrap</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style>
    img {
    padding: 20px; /* Adds 20px space on all sides */
    }
    </style>
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Binny the Trashbot</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#drawings">Drawings</a></li>
            <li><a href="#design">Design</a></li>
            <li><a href="#testing">Testing</a></li>
            <li><a href="#result">Result</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Binny the Trashbot</h1>
        <p class="lead">A Pi Powered Trash Can That Makes Garbage Fun!<br>By Dyllan Hofflich (drh253) and Laurence Lai (ll758)</p>
        <img class="img-rounded" src="pics/binny.jpg" alt="Binny the Trashbot" width=90% height=90%>
      </div>

      <hr>
      <div class="center-block">
          <h4 style="text-align:center;">Demonstration Video</h4>
          <iframe width="640" height="360" src="https://www.youtube.com/embed?v=et91Gea6CPk" frameborder="0" allowfullscreen></iframe>
      </div>

      <hr id="intro">

      <div style="text-align:center;">
              <h2>Introduction</h2>
              <p style="text-align: left;padding: 0px 30px;">Do you hate how tedious throwing trash away is and how boring it feels? We have the solution for you! Binny is an automated interactive trash can that encourages a greener world by following humans around and enticing them to throw away garbage with its quirky and demanding personality. Packed with a robust computer vision algorithm and fun interfaces like speakers for its voice, bright LED lights, clap sound detection, and a Wifi connected webpage, Binny is sure to make trash throwing exciting again!</p>
              <img class="img-rounded" src="pics/binny_angles.png" alt="Binny the Trashbot" width=90% height=90%>
              <p style="text-align: left;padding: 0px 30px;">Using a Raspberry Pi 4, Binny uses a USB webcam to run a YOLO object detection model to identify and track human objects nearby. Once detected, the Pi takes in the center xy coordinates of a detected person and commands directions to Binny's motorized wheel base to move towards a person. Ultrasonic senors are utilized for object avoidance behavior to prevent Binny from running into obstacles. A PiTFT screen is utilized to provide a real time monitor of Binny's object detection model as well as measured ultrasonic distances. Additionally, a sound sensor detects loud claps and triggers Binny to spin around and look for a person when idle. Bluetooth speaker connectivity is also provided to give life to Binny and provide him with a voice to interact with users and give updates on his tracking algorithm. Binny also hosts a webpage on its Wifi hotspot to toggle between manual and automated trash can control. Lastly, LEDs were included because they look cool and provide a sick startup animation!
              </p>
      </div>

    <hr id='obj'>

      <div class="row">
          <div class="col-md-8" style="font-size:18px;">
          <h2>Project Objectives</h2>
          <ul>
              <li>Develop a motorized base for a small trash can as well as a motor control interface.</li>
              <li>Use comptuer vision on the Pi to track and follow people based on xy coordinates.</li>
              <li>Uitlize ultrasoic sensors for obstacle avoidance behavior.</li>
              <li>Provide fun user interactivity features such as speakers, lights, and clap sound detection.</li>
              </ul>
          </div>
      </div>

    <hr id='drawings'>

      <div style="text-align:center;">
              <h2>Drawings</h2>
              <img class="img-round" src="pics/init_drawing.png" alt="Initial Propsosal" width=100%>
              <p style="text-align: left;padding: 0px 30px;">The image above showcases the first rudimentary mockup design of Binny. Trashbot was initally proposed to follow and catch trash thrown in the air in real time so it would always land in the bin. However, after initial software testing and disucssion with Dr. Ma and our TAs, the hardware limitations of the Pi make it extremely complex to have real time trajectory prediction and movement to catch flying trash in the air. Even after overclocking and lowering the complexity of our YOLO algorithm, the average FPS of our model stayed around 7. Note that in our initial drawings, we did not include additional features like ultrasonic sensors, lights, and clap sensors. These additions were formed throughout the development process in desire for a more interactive and fun project.</p>
      </div>
    
    <hr id='design'>

      <div style="text-align:center;">
            <h2>Design</h2>
              <h3>Mechanical Hardware</h3>
              <div style="display: flex; gap: 10px;">
                  <img class="img-round" src="pics/cad1.png" alt="Initial Proposal" style="width: 33%;">
                  <img class="img-round" src="pics/cad2.png" alt="Initial Proposal" style="width: 33%;">
                  <img class="img-round" src="pics/cad3.png" alt="Initial Proposal" style="width: 33%;">
              </div>
              <p style="text-align: left;padding: 0px 30px;">The images above show the Fusion 360 CAD mechanical model that we 3D printed for our project. Our goal for this design was to extend the bottom of an already existing trash can and neatly enclose all eletrical components. Notice that in our design, we kept motor mounts separate from the actual bin assembly to make printing easier and to allow for more rapid prototyping changes in case our motor mounts were incorrect (which happened many times). The extended bin base provides enough space to enclose a Pi 4, motor drivers, a long breadboard, and other tiny sensors such as the big sound sensor. A retangular hole is cut out in order to provide mounting for the PiTFT screen. The trash can is friction fit into the top of the extended base rim. A wide hole in the back provides open access to ports on the Pi and connections to battery chargers and the USB webcam mounted on top of the trashcan. Holes used in the design are made to fit M3 screws and nuts. Our final print was made out of PLA due to its low cost and faster printing time, as well as slightly more flexibility over more rigid materials like PETG.</p>
              
              <div style="display: flex; gap: 10px;">
                  <img class="img-round" src="pics/old_motor_mounts.png" alt="Initial Proposal" style="width: 50%;">
                  <img class="img-round" src="pics/new_motor_mounts.png" alt="Initial Proposal" style="width: 50%;">
              </div>

              <p style="text-align: left;padding: 0px 30px;"> One major mechanical issue we encountered in mechanical assembly were our motor mount designs. Initialy, our robot utilized larger RS550 12V DC motors but we eventually replaced our motors to use our Lab 3 DC gearbox motors. Since our motor mounts were a separate component from the extended bottom base, it was fairly simple to swap out these mounts. The images above showcase our motor swap, with the old RS550 design on the left and Lab 3 motor design on the right.</p>
              
              

              <h3>Electrical Hardware</h3>
              <img class="img-round" src="pics/circuit_image.png" alt="Initial Propsosal" width=100%>
              <div>
              <p style="text-align: left;padding: 0px 30px;">
              Binny contains a variety of electrical components with various uses. The breadboard diagram above shows the connections between the Raspberry Pi 4 and all electrical components used in the project. The image below the list showcases the physical implementation of all electrical components in our 3D printed bin enclousre. Binny contains the following major electrical components:
              <ul style="text-align: left;padding: 0px 30px;">
                <li>1 <b>Raspberry Pi 4</b> to control the entire system</li>
                <li>1 <b>solderless breadboard</b> to interconnect all components</li>
                <li>1 <b>PiTFT screen</b> to visualize computer vision data processing and ultrasonic distance measurements</li>
                <li>1 <b>Logitech USB webcam</b> to provide real time images to the Pi for object detection processsing</li>
                <li>2 <b>DC motors</b> for locomotion of the trash can</li>
                <li>1 <b>L298N DC motor drivers</b> for transferring low-voltage (3.3V) control signals to higher-voltage (12V) electrical outputs to the motors</li>
                <li>2 <b>ultrasonic distance sensors</b> to measure distances and avoid obstacles</li>
                <li>1 <b>big sound sensor</b> to sense for loud noises like claps</li>
                <li>1 <b>5V adressable LED strip</b></li>
                <li>1 <b>12V reachargable lithium ion</b> battery to power the motors and LED strip</li>
                <li>1 USB-C <b>portable battery</b> to power the Pi</li>
              </ul>
              </p>
              <img class="img-round" src="pics/IMG_2719.jpg" alt="Initial Propsosal" width=50%>
              </div>
              <br>

              <u style="text-align: left;padding: 0px 30px;">PiTFT Screen</u>
              <div>
              <img src="pics/girlypopscreen.png" alt="PiTFT screen" width=40%>
              <p style="text-align: left;padding: 0px 30px;">Binny uses the Adafruit PiTFT Plus, a 2.8-inch 320×240 resistive touchscreen display that connects directly to the Raspberry Pi’s GPIO header. The screen communicates over SPI, providing fast refresh rates suitable for UI elements, real-time debugging, and status monitoring. The compact size makes it ideal for embedding directly on the robot without adding bulk. The PiTFT allows us to view sensor outputs, run basic touch interfaces, and interact with the system without needing an external monitor, making on-device testing significantly more convenient during development.</p>
              </div>

              <u style="text-align: left;padding: 0px 30px;">Logitech USB Webcam</u>
              <div>
              <img src="pics/hd1080pllogitecusbwebcam.png" alt="Logitech USB Webcam" width=30%></img>
              <p style="text-align: left;padding: 0px 30px;">The Logitech C920 USB webcam serves as Binny’s main vision sensor. It was chosen for its wide-angle lens, high-quality 1080p video output, and strong performance in low-light environments—significantly better than the Raspberry Pi Camera Module in our testing. The C920 connects through USB and is fully supported by OpenCV, making it easy to integrate into future computer-vision features such as object detection, gesture recognition, or navigation assistance. Its plug-and-play reliability and superior optics make it an ideal choice for a mobile robot that may encounter different lighting conditions.</p>
              </div>

              <u style="text-align: left;padding: 0px 30px;">DC Gearbox Motors</u>
              <div>
              <img src="pics/tt_motor.jpg" alt="TT Motor" width=30%>
              <p style="text-align: left;padding: 0px 30px;">
                Binny uses two 12V DC gearbox motors from Lab 3 with one on each side (left and right) for locomotion. These motors are mounted at the bottom of the base near the back of the trashbin with a separate caster wheel in the front to maintain balance. The DC motors are generic TT DC gearbox motors with a gear ratio of 1:48. These motors are rated for 12V making them a good fit for power from four AA batteries. There is some variation in each motor due to the lack of encoders, speed control or position feedback. Regardless, these motors were chosen for their accessibiltiy with motor drivers, matching power requirements, and high torque value compared to previous DC motors we tried.
              </p>
              </div>
              
              <u style="text-align: left;padding: 0px 30px;">Motor Controller Module</u>
              <div>
              <img src="pics/l298n.webp" alt="L298N Motor Driver" width=45%>
              <p style="text-align: left;padding: 0px 30px;">
                Both motors are controlled by a dual-channel L298N motor driver. These drivers receive logic-level signals from the Raspberry Pi Pico W and translate them into higher-current outputs suitable for driving the motors. They are mounted inside the body of the dog near the front joints and the back joints. The motor driver provides control over motor speed using PWM and control over spin direction using an internal H-Bridge. Pulse Width Modulation (PWM) sends a series of ON-OFF pulses - the average voltage is proportional to the width or duty cycle of the pulses. The higher the duty cycle (larger pulse width), the higher the average voltage is applied to the DC motor therefore increasing the motor speed. The shorter the duty cycle, the lower the motor speed. The EN pin on each motor driver is pulsed using PWM. The H-Bridge controls the spin direction by changing the polarity of the input voltage. These are controlled by the InA and InB pins on the motor driver. Battery power is applied to the Vs and GND pins to supply the DC motors.
              </p>
              </div>

              <u style="text-align: left;padding: 0px 30px;">Ultrasonic Distance Sensors</u>
              <div>
              <img src="pics/ultrasonic.png" alt="Ultrasonic" width=45%>
              <p style="text-align: left;padding: 0px 30px;">
                Two HC-SR04 ultrasonic distance sensors mounted at the front of the robot measure distances to nearby objects. which take in DC 5V and have an ultrasonic frequency of 40 kHz. It has a minimum range of 2cm and a maximum range of 400cm, with an accuracy of 3mm and a measuring angle of less than 15 degrees. Ultrasonic sensors measure the distance to an object by using a transducer to send and receive ultrasonic pulses that relay information about an object's proximity. A sound wave at a frequency above human hearing range is sent out using the Trig pin and then the Echo pin listens for the reflected signal. The software subsection describes the process of converting this information into recordable measurements. These sensors form the core of the obstacle avoidance software.
              </p>
              </div>

              <u style="text-align: left;padding: 0px 30px;">Big Sound Sensor</u>
              <div>
              <img src="pics/big-sound.png" alt="Big Sound Sensor" width=30%></img>
              <p style="text-align: left;padding: 0px 30px;">
                The big sound sensor operates at 3.3V and is used primarily for clap-based activation or attention cues. It includes a built-in microphone and comparator circuit that outputs a digital HIGH signal when ambient sound crosses a defined threshold. This makes it well-suited for detecting sharp noises, such as claps, taps, or loud commands. The sensor interfaces cleanly with the Raspberry Pi's 3.3V GPIO pins without the need for additional level shifting.
              </p>
              </div>

              <u style="text-align: left;padding: 0px 30px;">LED Strip</u>
              <div>
              <img src="pics/ledstrip.png" alt="LED Strip" width=30%></img>
              <p style="text-align: left;padding: 0px 30px;">
                A 5V WS2812 individually-addressable RGB LED strip is mounted around Binny to provide visual feedback and personality through lighting patterns. Each LED contains an internal IC that allows color and brightness to be controlled independently through a single-wire data protocol. The strip is powered by the motor driver's built-in 5V regulator output on the L298N motor driver, ensuring stable voltage levels. The Raspberry Pi drives the data line to control animations, indicators, and status signals such as obstacle alerts, battery warnings, or expressive lighting patterns. This strip offers flexible color control while remaining power-efficient for mobile operation.
              </p>
              </div>

              <u style="text-align: left;padding: 0px 30px;">Power</u>
              <div>
              <p style="text-align: left;padding: 0px 30px;">
              Binny consists of two power supplies - one 12V rechargable Li-ion battery and one portable battery providing USB power. The Li-ion battery provides 12V at around 3000 mAH of power at full charge. We chose to go with this battery as it was rechargable and met the power requirements for the motors used in this project. In addition to powering the motors, the motor driver used includes a built in 12V to 5V buck converter, which we use to power our 5V LED strip. Additionally, taking 5V from this battery prevents undervotlage power warnings on the Pi. This takes power load off the Raspberry Pi, allowing maximum power usage to go towards software processes rather than hardware requirements. The portable battery bank provides USB power to the Raspberry Pi. USB power is advantageous over a battery as it provides a regulated power source and prevents overvoltage or undervoltage of the Pi which could negatively harm processes. 
              </p>
              </div>

              <b>GPIO Mapping</b>
              <div style="display: flex; justify-content: center; margin-top: 10px;">
              <style type="text/css">
              .tg  {border-collapse:collapse;border-spacing:0;}
              .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
                overflow:hidden;padding:10px 5px;word-break:normal;}
              .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
                font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
              .tg .tg-0lax{text-align:center;vertical-align:top}
              </style>
              <table class="tg"><thead>
                <tr>
                  <th class="tg-0lax"><span style="font-weight:bold">GPIO</span></th>
                  <th class="tg-0lax"><span style="font-weight:bold">Function</span></th>
                </tr></thead>
              <tbody>
                <tr>
                  <td class="tg-0lax">4</td>
                  <td class="tg-0lax">Big Sound Sensor Data</td>
                </tr>
                <tr>
                  <td class="tg-0lax">12</td>
                  <td class="tg-0lax">LED Strip Data</td>
                </tr>
                <tr>
                  <td class="tg-0lax">13</td>
                  <td class="tg-0lax">Top Ultrasonic Sensor Echo </td>
                </tr>
                <tr>
                  <td class="tg-0lax">19</td>
                  <td class="tg-0lax">Top Ultrasonic Sensor Trig</td>
                </tr>
                <tr>
                  <td class="tg-0lax">22</td>
                  <td class="tg-0lax">Bottom Ultrasonic Sensor Echo </td>
                </tr>
                <tr>
                  <td class="tg-0lax">27</td>
                  <td class="tg-0lax">Bottom Ultrasonic Sensor Trig</td>
                </tr>
                <tr>
                  <td class="tg-0lax">26</td>
                  <td class="tg-0lax">Left Motor PWM</td>
                </tr>
                <tr>
                  <td class="tg-0lax">5</td>
                  <td class="tg-0lax">Left Motor Direction A</td>
                </tr>
                <tr>
                  <td class="tg-0lax">6</td>
                  <td class="tg-0lax">Left Motor Direction B</td>
                </tr>
                <tr>
                  <td class="tg-0lax">16</td>
                  <td class="tg-0lax">Right Motor PWM</td>
                </tr>
                <tr>
                  <td class="tg-0lax">17</td>
                  <td class="tg-0lax">Right Motor Direction A</td>
                </tr>
                <tr>
                  <td class="tg-0lax">20</td>
                  <td class="tg-0lax">Right Motor Direction B</td>
                </tr>
              </tbody></table>
              </div>

              <h3>Software</h3>

              <img class="img-round" src="pics/trashbot_software.png" alt="Software Architecture" width=70%>

              <p style="text-align: left;padding: 0px 30px;">
              Binny has a variety of softaware components that work together to provide an enjoyable experience to throwing out trash. A rough diagram of our software architecture is shown above. A more detailed description of each software component is provided below.
              </p>

              <h4>Computer Vision</h4>
              
              <h5> YOLO Model Optimization</h5>
              <p style="text-align: left;padding: 0px 30px;">
              To enable Binny to follow humans around, we implemented a computer vision algorithm using the YOLO (You Only Look Once) object detection model. 
              We chose YOLO for its speed and accuracy, 
              making it suitable for real-time applications on the Raspberry Pi, and because initially, we were aiming to detect a variety of objects as they flew in the air in real-time, however, despite our best efforts, the Raspberry Pi's CPU is not fast enough to do so.
              
              We used a pre-trained YOLOv11n_ncnn model. The model was optimized to run efficiently on the 
              Raspberry Pi by reducing its size and complexity, allowing Binny to process video frames at a reasonable frame rate. The n in YOLOv11n stands for nano, being the smallest version of the latest YOLO fleet. By using the YOLOv11n model, we can only achieve ~1.5FPS, which is too slow for any of our applications. To further improve it, we overclocked our Pi from 1.5GHz to 2GHz and raising the voltage of the CPU from 0.8V to 0.95V to speed up the processing. This raised the FPS slightly to 2, but was still insufficient. We then also exported the model to NCNN, which is a neural network framework that is optimized for embedded devices such as a Raspberry Pi (https://docs.ultralytics.com/integrations/ncnn/#usage). Using the framework directly didn't lead to much of an increase in speed, but while exporting the model, we can also change the model's image size from 640x640px to 320x320px, and to use floating point 16 instead of 32 to scarifice precision and image resolution for higher speeds. This raised the speed to 7FPS. We use the Ultralytics Python library as a wrapper to import, export, and call the Pytorch YOLO model. 
              </p>

              <h5>Human Detection and Tracking</h5>
              <p style="text-align: left;padding: 0px 30px;">
              We use OpenCV to capture video frames via the USB camera, which are then processed by the 
              YOLO model to detect humans. The model outputs bounding boxes around detected humans along with their confidence scores. We extract 
              the center coordinates of the bounding boxes 
              to determine the position of the person relative to Binny. These coordinates are then used to calculate the direction and speed at which 
              Binny should move to follow the detected person. We had a major problem with the model detecting irrelevant objects such as chairs and backpacks, which would cause Binny to get stuck or move erratically. To solve this, we set a confidence score threshold of 0.7 and only consider detections with a confidence score above this threshold as valid human detections. This significantly improved the reliability of Binny's tracking behavior.
              Additionally, we implemented a simple proportional controller to adjust Binny's movement based on the horizontal offset of the detected person from the center of the camera frame. As a person approaches a the middle area of the camera, about 40 pixels from the middle, the motors will stop moving left/right, but as one deviates from this area, the motors will adjust their speed accordingly. This allows Binny to smoothly follow the person. The exact motor speeds and thresholds were determined through extensive testing and tuning. We opted to settle on a very low turning speed, allowing the robot to travel straight more easily without veering off course, but at the cost of slower turning response, and not being able to follow sharp turns when you are close.
              The forwards motion of Binny is determined by the size of the bounding box, which correlates to the distance of the person from Binny. As the person gets closer, the bounding box size increases, and Binny slows down to avoid collisions. If the bounding box size exceeds a certain threshold, Binny will stop moving forward altogether. As a safety measure, if no humans are detected for more than 2 seconds, Binny will stop moving to prevent wandering off. 
              </p>

              <h5>PiTFT display</h5>
              <img class="img-round" src="pics/lcdScreen.png" alt="PiTFT Display" width=70%>
              <p style="text-align: left;padding: 0px 30px;">
              The PiTFT display is used to provide a real-time visual interface for Binny as shown above. It displays the video feed from the USB camera along with the bounding boxes drawn around detected humans. This allows users to see what Binny is "seeing" and how it is interpreting its surroundings. The display also shows additional information such as the current FPS, distance readings from the ultrasonic sensors, and how many objects it detects in a scene. 
              The PiTFT is interfaced directly by writing to the framebuffer using OpenCV, allowing for efficient rendering of graphics and text. This direct framebuffer access was actually a major source of lag in our system, as writing to the PiTFT's framebuffer is quite slow. To mitigate this, we optimized our drawing routines to only open the frame buffer once as opposed to opening and closing it every frame. This significantly improved the overall performance of the display, and the overall program since this issue was causing the entire system to lag.
              </p>

              <h4>Motor Control</h4>
              <p style="text-align: left;padding: 0px 30px;">
              Motor control is achieved using the pigpio library to interface with the GPIO pins of the Raspberry Pi. The library allows for precise control of PWM signals to drive the motor driver, which in turn controls the speed and direction of the DC motors. We set up two motors for differential drive, allowing Binny to move forward, backward, and turn by varying the speed of each motor independently. The motor speeds are adjusted based on the output from the computer vision algorithm, allowing Binny to follow detected humans smoothly. The interface for these motors are similar to the one used in Lab 3 of this class, with functions to alter the PWM duty cycle and direction of each motor as we want to change speed and direction.
              </p>

              <h4>Ultrasonic Sensors</h4>
              <p style="text-align: left;padding: 0px 30px;">
              Ultrasonic sensors are used for obstacle detection and avoidance. We use two ultrasonic sensors mounted on the front of Binny to measure distances to nearby objects. The sensors work by sending out a high-frequency sound pulse and measuring the time it takes for the echo to return. This time is then converted into a distance measurement. The pigpio library is used to generate the trigger pulse and measure the echo pulse duration using GPIO pins. If an object is detected within a certain distance of 10cm, Binny will stop moving forward to avoid collisions. The threshold of 10cm was decided after extensive tests and hard hits to the LCD screen (hence the bubble wrap). This safety feature ensures that Binny can navigate its environment without running into obstacles, and also serves as a way for someone to signal Binny to back up if it gets too close.
              We have the ultrasonic sensors constantly measuring distances in a separate thread, updating global distance variables that the main control loop can access. This allows for real-time obstacle detection without blocking the rest of the program from running.
            </p>

              <h4>Clap Detection</h4>
              <p style="text-align: left;padding: 0px 30px;">
              Clap detection is implemented using a sound sensor connected to the Raspberry Pi. The sensor outputs an digital signal when a loud sound, such as a clap, is detected. We use the pigpio library to monitor the GPIO pin connected to the sound sensor for rising edges, indicating a clap event. When a clap is detected, Binny's main loop will interupt and it will spin around in place to look for a person to follow. This feature adds an interactive element to Binny, allowing users to easily get its attention when it is idle, or make Binny turn around if it ends up facing the wrong direction. The spin duration and speed were determined through testing to ensure Binny can effectively reorient itself without overshooting. We opted for a smaller spin angle of about 10 degrees per clap to prevent Binny from getting disoriented and losing track of the person it was following, but since we are controlling the motors directly, the angles change depending on what floor Binny is on.
              We also had to tune the sensitivity of the sound sensor to avoid false positives from ambient noise. This was done by adjusting the potentiometer on the sound sensor module.
              </p>

              <h4>LEDs</h4>
              <img class="img-round" src="pics/leds.png" alt="LEDs" width=45%>
              <p style="text-align: left;padding: 0px 30px;">
              The LEDs are used to provide visual feedback and enhance the user experience. We use a NeoPixel ring connected to the Raspberry Pi via a dedicated GPIO pin. The Adafruit NeoPixel library is used to control the LEDs, allowing us to set colors and animations. At startup, Binny performs a colorful LED animation to indicate that it is powering on. During operation, the LEDs can change colors based on Binny's state, such as: wiping a red across the strip if it is within range of a person to signify stopping, flashing a blue light when it hears a clap to turn, flashing a green light when it chases someone, wiping white across the strip when we transition from joystick to autonomous mode, and having a rainbow gradient fade on the strip when we are controlling the robot using a joystick. 
              The LED effects are managed in a separate python process from the main app since the NeoPixel library needs to be run as root, which the rest of the program was not configured to do (we ran into permission issues when running the ultrasonic code as root). Communication between the main app and the LED process is done via a simple command line protocol using the subprocess library. We can run a command such as "sudo -E python light_test.py --mode wipe --color 0,255,0" from our main Python program to spawn a process that is run as root to handle the LEDs. This allows us to change LED effects in real-time based on Binny's state without stopping the main program flow.
              A problem we encountered with the LEDs is that if the LED process is not properly terminated, the pins controlling the LEDs can get stuck in their last state, and conflict with any subsequent calls, causing the whole system to lag and the LEDs to flash erratically or just not change color. To solve this, we implemented a cleanup routine that is called when the main program exits, which sends a command to the LED process to properly de-initialize the pin used to control the LEDs. Additionally, we made sure to prevent multiple LED processes from running simultaneously by checking for existing processes before spawning a new one, and by limiting the time between LED processes to at least 5 seconds.
              The large gap between LED commands is to prevent multiple LED processes from running at once, which would cause the system to lag and the LEDs to behave erratically.
              Additionally, to run the LED process at startup with a set animation, we have a bash script to run a separate program that handles the LED startup animation, that then runs the main Binny program after the animation is complete.
              A nother large problem we had with these LEDs is that they draw a relatively a lot of voltage, which when powered directly from the Pi led to issues with controlling the camera, screen, and motors all at once which already is taxing. To solve this, we powered the LEDs from a separate 5V power supply via the motor driver to offload the power draw from the Pi.

              <h4>Website and Hotspot</h4>
              <p style="text-align: left;padding: 0px 30px;">
              The website serves as a user interface for controlling Binny. It is hosted on the Raspberry Pi using the Flask web framework. The website allows users to toggle between manual joystick control and autonomous mode. In joystick mode, users can control Binny's movement using an on-screen joystick implemented with the nipplejs library. In autonomous mode, Binny uses its computer vision and obstacle avoidance algorithms to follow humans automatically. The website has a button to switch modes, which sends a request to the Flask server to update Binny's operating mode. The server then communicates this change to the main control loop of Binny, allowing it to switch between manual and autonomous operation.
              Additionally, The Raspberry Pi is configured to create a WiFi hotspot using networkmanager, allowing users to connect directly to Binny's network. This enables easy access to the website without needing an external WiFi network. The hotspot configuration includes setting up a static IP address for the Pi so users can access the website at a known address (10.42.0.1:5000).
              The main draw of this is that it allows you to control Binny using just your phone from anywhere within WiFi range, making it very convenient to use. It also allows you to control Binny manually if the can is stuck somewhere, not following you properly, or is just fairly far away.
              </p>
              <grid>
              <img class="img-round" src="pics/joystickModeWebsite.png" alt="Website" width=45%>
              <img class="img-round" src="pics/autoModeWebsite.png" alt="Website" width=45%>
              </grid>

              <h4>Speaker Voice</h4>
              <p style="text-align: left;padding: 0px 30px;"></p>
              Binny automatically connects to a bluetooth speaker upon startup and on run of our demo Python file through a custom bash script named <b>autopair</b>. Using Linux command bluetoothctl, we first initially pair and trust the connected bluetooth speaker by searching for its MAC address using the scan function of bluetoothctl. Once the bluetooth speaker has been trusted, a bash script which starts bluetoothctl and connects to the identified MAC address is configured to run at boot and at the start of our demo Python file. Note that the bluetooth speaker must be on and in pairing mode beforehand. 

              We then use the bluetooth speaker to give audio feedback to the user using the espeak text-to-speech command line tool. 
              Binny uses espeak to announce its current state, such as when it starts following a person (saying "wait up"), when it stops due to an obstacle (saying "ahhh too close"), when it reaches you (saying "got it"), when it spins due to a clap and looks for you (saying "where are you trash), or when it switches between manual and autonomous modes (saying "joystick/autonomous mode"). The espeak command is called from the main Python, letting the program continue to run with the espeak process in the background.

              </p>
              


            
              <!-- <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p> -->
      </div>

    <hr id='testing'>

      <div style="text-align:center;">
              <h2>Testing</h2>
              motor speed differences (not going straight),
              motor wiring issues
              motor driver issues
              improving how to turn towards person
              improving confidence score threshold
              Pitft and espeak process laggings
              led not cleaning up when finished

              <p>
                In order to ensure the reliability and performance of Binny, we conducted a series of tests focusing on various aspects of the system. These tests included:
              </p>
              <p style="text-align: left;padding: 0px 30px;">
              1. Motor Functionality: 
              We tested the motors individually to ensure they responded correctly to speed and direction commands. 
              This involved running the motors via the Lab 3 rolling code to verify their operation before integrating them into Binny's control system.
              At first, we had 12V motors that were too weak to move the entire trash can, so we switched to stronger motors from Lab 3 that used a gearbox to provide more torque. We were worried that they would not be able to provide enough speed, but after testing, we found that they were sufficient for our needs, in fact sometimes they were too fast and flipped the can over, so we had to limit the speed in software.
              We were also worried that the motors would not be able to handle the weight of the trash can, and the higher voltage of 12V since they were only rated for 8V but after testing, we found that they were able to move the can without any issues.
              We also calibrated the motors to ensure straight-line movement, making each motor spin at the same duty cycle and seeing if it actually went straight. Whether it was because of slight differences in the motors, wheel alignment, or the weight distribution of the trash can, we found that one motor would often spin slightly faster than the other, causing Binny to veer off course. To fix this, we adjusted the PWM duty cycle of each motor until Binny could move in a straight line reliably.
              <br>
              2. Computer Vision Accuracy: 
              While running our program, we saw that sometimes the model would detect irrelevant objects such as chairs and backpacks, which would cause Binny to get stuck or move erratically. To solve this, we set a confidence score threshold of 0.7 and only consider detections with a confidence score above this threshold as valid human detections. This significantly improved the reliability of Binny's tracking behavior. Additionally, we added a buffer of two frames where if no humans are detected for two consecutive frames, Binny will stop moving to prevent wandering off, but if only one frame has no detections, it will continue moving as normal. This helps to prevent false negatives from causing Binny to stop unnecessarily.
              Additionally, we tested Binny's ability to follow a person by having someone walk in front of it at various speeds and distances. We observed Binny's responsiveness and adjusted the motor speed scaling factors to ensure that it could sort-of follow someone or go to a stationary person without overshooting. While this is not perfect, we found that Binny could generally move towards a stationary person, or follow a very slow moving person when they don't make sharp turns. The robot can't make sharp turns because its frame rate is 7FPS, and therefore update rate is only 0.14 seconds, which means that if the person makes a sharp turn, Binny will not be able to react in time and will overshoot. We tried to mitigate this by lowering the turning speed of the motors, allowing Binny to travel straight more easily without veering off course, but at the cost of slower turning response.
              <br>
              3. Obstacle Avoidance: 
              We tested the ultrasonic sensors' effectiveness in detecting obstacles and preventing collisions. This included avoiding tough objects such as chairs with a metal bar that juts out a foot off the ground, as well as other objects such as peoples feet, walls, and tables. The chair problem is why we had to double the amount of ultrasonic sensors, becuase when testing it would always ram its screen into the chairs because of either its legs or the metal bar. We found that having two ultrasonic sensors at different heights allowed Binny to better detect obstacles and avoid collisions. We found that 10cm is a good threshold distance to stop Binny from moving forward, as it allows enough time for Binny to react and stop before hitting an obstacle.
              <br>
              4. Clap Detection: We assessed the sound sensor's sensitivity and responsiveness to claps. This involved testing in various noise environments to ensure reliable detection without false positives, such as noisy labs with other students. We adjusted the potentiometer on the sound sensor module to tune its sensitivity, finding a balance that allowed for consistent clap detection while minimizing false triggers from ambient noise. We also saw that the motors would cause the sound sensor to go off. To solve this, we only looked for claps after 5 frames of no person detected so 5 frames after it stops moving. During testing, we found that Binny could reliably detect claps from across the room (given it was a loud clap), allowing users to easily get its attention. We also tested the spin behavior upon clap detection, ensuring that Binny could effectively reorient itself without overshooting or getting disoriented. We opted for having a very small angle of about 10 degrees per clap to make sure it wouldn't overshoot.
              5. LED Functionality: We verified that the LED strip responded correctly to commands and displayed the intended colors and animations.
              <br>
              6. Website Control: We tested the website interface to ensure seamless switching between manual and autonomous modes, as well as joystick responsiveness.
              7. Power Management: We monitored the power consumption of the system to ensure that the batteries could sustain operation for a reasonable duration. During all of our testing, the battery packs lasted for multiple hours without needing a recharge.
              8. Lag and Performance: We measured the overall system performance, including frame rates for computer vision processing and responsiveness of motor control. We identified and mitigated sources of lag, such as optimizing framebuffer access for the PiTFT display and managing LED and espeak control processes to prevent conflicts. We found that when the ultrasonic sensor was going off constantly, it would slow down the entire system due to each close detection causing a speak process, and the ultrasonic thread ran at a very high frequency, leading to hundreds of espeak processes being spawned. To solve this, we limited the frequency of espeak calls to start another process if the last one has finished, leading to a significant performance increase.
              </p>
              


      </div>

    <hr id='result'>

      <div style="text-align:center;">
              <h2>Result</h2>
              <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p>
      </div>

    <hr>

    <div class="row" style="text-align:center;">
          <h2>Work Distribution</h2>
          <div style="text-align:center;">
              <img class="img-rounded" src="pics/group.jpg" alt="Generic placeholder image" style="width:80%;">
              <h4>Project group picture</h4>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/a.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Rick</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Designed the overall software architecture (Just being himself).
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/b.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Morty</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Tested the overall system.
          </div>
      </div>

    <hr>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li>Raspberry Pi $35.00</li>
              <li>Raspberry Pi Camera V2 $25.00</li>
              <a href="https://www.adafruit.com/product/1463"><li>NeoPixel Ring - $9.95</li></a>
              <li>LEDs, Resistors and Wires - Provided in lab</li>
          </ul>
          <h3>Total: $69.95</h3>
      </div>
      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
          <a href="http://getbootstrap.com/">Bootstrap</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>

      </div>

    <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>
