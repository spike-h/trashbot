
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Starter Template for Bootstrap</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Binny the Trashbot</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#drawings">Drawings</a></li>
            <li><a href="#design">Design</a></li>
            <li><a href="#testing">Testing</a></li>
            <li><a href="#result">Result</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Binny the Trashbot</h1>
        <p class="lead">A Pi Powered Trash Can That Makes Garbage Fun!<br>By Dyllan Hofflich (drh253) and Laurence Lai (ll758)</p>
        <img class="img-rounded" src="pics/binny.jpg" alt="Binny the Trashbot" width=90% height=90%>
      </div>

      <hr>
      <div class="center-block">
          <h4 style="text-align:center;">Demonstration Video</h4>
          <iframe width="640" height="360" src="https://www.youtube.com/embed?v=et91Gea6CPk" frameborder="0" allowfullscreen></iframe>
      </div>

      <hr id="intro">

      <div style="text-align:center;">
              <h2>Introduction</h2>
              <p style="text-align: left;padding: 0px 30px;">Do you hate how tedious throwing trash away is and how boring it feels? We have the solution for you! Binny is an automated interactive trash can that encourages a greener world by following humans around and enticing them to throw away garbage with its quirky and demanding personality. Packed with a robust computer vision algorithm and fun interfaces like speakers for its voice, bright LED lights, clap sound detection, and a Wifi connected webpage, Binny is sure to make trash throwing exciting again!</p>
              <img class="img-rounded" src="pics/binny_angles.png" alt="Binny the Trashbot" width=90% height=90%>
              <p style="text-align: left;padding: 0px 30px;">Using a Raspberry Pi 4, Binny uses a USB webcam to run a YOLO object detection model to identify and track human objects nearby. Once detected, the Pi takes in the center xy coordinates of a detected person and commands directions to Binny's motorized wheel base to move towards a person. Ultrasonic senors are utilized for object avoidance behavior to prevent Binny from running into obstacles. A PiTFT screen is utilized to provide a real time monitor of Binny's object detection model as well as measured ultrasonic distances. Additionally, a sound sensor detects loud claps and triggers Binny to spin around and look for a person when idle. Bluetooth speaker connectivity is also provided to give life to Binny and provide him with a voice to interact with users and give updates on his tracking algorithm. Binny also hosts a webpage on its Wifi hotspot to toggle between manual and automated trash can control. Lastly, LEDs were included because they look cool and provide a sick startup animation!
              </p>
      </div>

    <hr id='obj'>

      <div class="row">
          <div class="col-md-8" style="font-size:18px;">
          <h2>Project Objectives</h2>
          <ul>
              <li>Develop a motorized base for a small trash can as well as a motor control interface.</li>
              <li>Use comptuer vision on the Pi to track and follow people based on xy coordinates.</li>
              <li>Uitlize ultrasoic sensors for obstacle avoidance behavior.</li>
              <li>Provide fun user interactivity features such as speakers, lights, and clap sound detection.</li>
              </ul>
          </div>
      </div>

    <hr id='drawings'>

      <div style="text-align:center;">
              <h2>Drawings</h2>
              <img class="img-round" src="pics/init_drawing.png" alt="Initial Propsosal" width=100%>
              <p style="text-align: left;padding: 0px 30px;">The image above showcases the first rudimentary mockup design of Binny. Trashbot was initally proposed to follow and catch trash thrown in the air in real time so it would always land in the bin. However, after initial software testing and disucssion with Dr. Ma and our TAs, the hardware limitations of the Pi make it extremely complex to have real time trajectory prediction and movement to catch flying trash in the air. Even after overclocking and lowering the complexity of our YOLO algorithm, the average FPS of our model stayed around 7. Note that in our initial drawings, we did not include additional features like ultrasonic sensors, lights, and clap sensors. These additions were formed throughout the development process in desire for a more interactive and fun project.</p>
      </div>
    
    <hr id='design'>

      <div style="text-align:center;">
            <h2>Design</h2>
              <h3>Mechanical Hardware</h3>
              <div style="display: flex; gap: 10px;">
                  <img class="img-round" src="pics/cad1.png" alt="Initial Proposal" style="width: 33%;">
                  <img class="img-round" src="pics/cad2.png" alt="Initial Proposal" style="width: 33%;">
                  <img class="img-round" src="pics/cad3.png" alt="Initial Proposal" style="width: 33%;">
              </div>
              <p style="text-align: left;padding: 0px 30px;">The images above show the Fusion 360 CAD mechanical model that we 3D printed for our project. Our goal for this design was to extend the bottom of an already existing trash can and neatly enclose all eletrical components. Notice that in our design, we kept motor mounts separate from the actual bin assembly to make printing easier and to allow for more rapid prototyping changes in case our motor mounts were incorrect (which happened many times). The extended bin base provides enough space to enclose a Pi 4, motor drivers, a long breadboard, and other tiny sensors such as the big sound sensor. A retangular hole is cut out in order to provide mounting for the PiTFT screen. The trash can is friction fit into the top of the extended base rim. A wide hole in the back provides open access to ports on the Pi and connections to battery chargers and the USB webcam mounted on top of the trashcan. Holes used in the design are made to fit M3 screws and nuts. Our final print was made out of PLA due to its low cost and faster printing time, as well as slightly more flexibility over more rigid materials like PETG.</p>
              
              <div style="display: flex; gap: 10px;">
                  <img class="img-round" src="pics/old_motor_mounts.png" alt="Initial Proposal" style="width: 30%;">
                  <img class="img-round" src="pics/new_motor_mounts.png" alt="Initial Proposal" style="width: 70%;">
              </div>

              <p style="text-align: left;padding: 0px 30px;"> One major mechanical issue we encountered in mechanical assembly were our motor mount designs. Initialy, our robot utilized larger RS550 12V DC motors but we eventually replaced our motors to use our Lab 3 DC gearbox motors. Since our motor mounts were a separate component from the extended bottom base, it was fairly simple to swap out these mounts. The images above showcase our motor swap, with the old RS550 design on the left and Lab 3 motor design on the right.</p>
              
              

              <h3>Electrical Hardware</h3>
              <img class="img-round" src="pics/circuit_image.png" alt="Initial Propsosal" width=100%>
              Binny contains a wide variety of electrical components, each with their own functionality. The breadboard diagram 

              <h3>Software</h3>
              <li> FLOW CHART OF CONTROL SCHEME === joystick mode ==== auto +> wait up, got it, backing up, where are you trash, just stopped</li>
              <li>Computer Vision: (overclocking and overvoltage, shrinking model, detection scheme, camera) -- CHECK</li>
              <li>PiTFT display - CHECK</li>
              <li>motor control - CHECK </li>
              <li>ultrasonic sensing - CHECK</li>
              <li>clap detection - CHECK</li>
              <li>LEDs</li>
              <li>website and hotspot</li>
              <li>speaker voice</li>
              

              <h4>Computer Vision</h4>
              
              <h5> YOLO Model Optimization</h5>
              <p style="text-align: left;padding: 0px 30px;">
              To enable Binny to follow humans around, we implemented a computer vision algorithm using the YOLO (You Only Look Once) object detection model. 
              We chose YOLO for its speed and accuracy, 
              making it suitable for real-time applications on the Raspberry Pi, and because initially, we were aiming to detect a variety of objects as they flew in the air in real-time, however, despite our best efforts, the Raspberry Pi's CPU is not fast enough to do so.
              
              We used a pre-trained YOLOv11n_ncnn model. The model was optimized to run efficiently on the 
              Raspberry Pi by reducing its size and complexity, allowing Binny to process video frames at a reasonable frame rate. The n in YOLOv11n stands for nano, being the smallest version of the latest YOLO fleet. By using the YOLOv11n model, we can only achieve ~1.5FPS, which is too slow for any of our applications. To further improve it, we overclocked our Pi from 1.5GHz to 2GHz and raising the voltage of the CPU from 0.8V to 0.95V to speed up the processing. This raised the FPS slightly to 2, but was still insufficient. We then also exported the model to NCNN, which is a neural network framework that is optimized for embedded devices such as a Raspberry Pi (https://docs.ultralytics.com/integrations/ncnn/#usage). Using the framework directly didn't lead to much of an increase in speed, but while exporting the model, we can also change the model's image size from 640x640px to 320x320px, and to use floating point 16 instead of 32 to scarifice precision and image resolution for higher speeds. This raised the speed to 7FPS. We use the Ultralytics Python library as a wrapper to import, export, and call the Pytorch YOLO model. 
              </p>

              <h5>Human Detection and Tracking</h5>
              <p style="text-align: left;padding: 0px 30px;">
              We use OpenCV to capture video frames via the USB camera, which are then processed by the 
              YOLO model to detect humans. The model outputs bounding boxes around detected humans along with their confidence scores. We extract 
              the center coordinates of the bounding boxes 
              to determine the position of the person relative to Binny. These coordinates are then used to calculate the direction and speed at which 
              Binny should move to follow the detected person. We had a major problem with the model detecting irrelevant objects such as chairs and backpacks, which would cause Binny to get stuck or move erratically. To solve this, we set a confidence score threshold of 0.7 and only consider detections with a confidence score above this threshold as valid human detections. This significantly improved the reliability of Binny's tracking behavior.
              Additionally, we implemented a simple proportional controller to adjust Binny's movement based on the horizontal offset of the detected person from the center of the camera frame. As a person approaches a the middle area of the camera, about 40 pixels from the middle, the motors will stop moving left/right, but as one deviates from this area, the motors will adjust their speed accordingly. This allows Binny to smoothly follow the person. The exact motor speeds and thresholds were determined through extensive testing and tuning. We opted to settle on a very low turning speed, allowing the robot to travel straight more easily without veering off course, but at the cost of slower turning response, and not being able to follow sharp turns when you are close.
              The forwards motion of Binny is determined by the size of the bounding box, which correlates to the distance of the person from Binny. As the person gets closer, the bounding box size increases, and Binny slows down to avoid collisions. If the bounding box size exceeds a certain threshold, Binny will stop moving forward altogether. As a safety measure, if no humans are detected for more than 2 seconds, Binny will stop moving to prevent wandering off. 
              </p>

              <h5>PiTFT display</h5>
              <p style="text-align: left;padding: 0px 30px;">
              The PiTFT display is used to provide a real-time visual interface for Binny. It displays the video feed from the USB camera along with the bounding boxes drawn around detected humans. This allows users to see what Binny is "seeing" and how it is interpreting its surroundings. The display also shows additional information such as the current FPS, distance readings from the ultrasonic sensors, and how many objects it detects in a scene. 
              The PiTFT is interfaced directly by writing to the framebuffer using OpenCV, allowing for efficient rendering of graphics and text. This direct framebuffer access was actually a major source of lag in our system, as writing to the PiTFT's framebuffer is quite slow. To mitigate this, we optimized our drawing routines to only open the frame buffer once as opposed to opening and closing it every frame. This significantly improved the overall performance of the display, and the overall program since this issue was causing the entire system to lag.
              </p>

              <h4>Motor Control</h4>
              <p style="text-align: left;padding: 0px 30px;">
              Motor control is achieved using the pigpio library to interface with the GPIO pins of the Raspberry Pi. The library allows for precise control of PWM signals to drive the motor driver, which in turn controls the speed and direction of the DC motors. We set up two motors for differential drive, allowing Binny to move forward, backward, and turn by varying the speed of each motor independently. The motor speeds are adjusted based on the output from the computer vision algorithm, allowing Binny to follow detected humans smoothly. The interface for these motors are similar to the one used in Lab 3 of this class, with functions to alter the PWM duty cycle and direction of each motor as we want to change speed and direction.
              </p>

              <h4>Ultrasonic Sensors</h4>
              <p style="text-align: left;padding: 0px 30px;">
              Ultrasonic sensors are used for obstacle detection and avoidance. We use two ultrasonic sensors mounted on the front of Binny to measure distances to nearby objects. The sensors work by sending out a high-frequency sound pulse and measuring the time it takes for the echo to return. This time is then converted into a distance measurement. The pigpio library is used to generate the trigger pulse and measure the echo pulse duration using GPIO pins. If an object is detected within a certain distance of 10cm, Binny will stop moving forward to avoid collisions. The threshold of 10cm was decided after extensive tests and hard hits to the LCD screen. This safety feature ensures that Binny can navigate its environment without running into obstacles, and also serves as a way for someone to signal Binny to back up if it gets too close.
              We have the ultrasonic sensors constantly measuring distances in a separate thread, updating global distance variables that the main control loop can access. This allows for real-time obstacle detection without blocking the main program flow.
            </p>

              <h4>Clap Detection</h4>
              <p style="text-align: left;padding: 0px 30px;">
              Clap detection is implemented using a sound sensor connected to the Raspberry Pi. The sensor outputs an digital signal when a loud sound, such as a clap, is detected. We use the pigpio library to monitor the GPIO pin connected to the sound sensor for rising edges, indicating a clap event. When a clap is detected, Binny's main loop will interupt and it will spin around in place to look for a person to follow. This feature adds an interactive element to Binny, allowing users to easily get its attention when it is idle. The spin duration and speed were determined through testing to ensure Binny can effectively reorient itself without overshooting. We opted for a smaller spin angle of about 10 degrees per clap to prevent Binny from getting disoriented and losing track of the person it was following, but since we are controlling the motors directly, the angles change depending on what floor Binny is on.
              We also had to tune the sensitivity of the sound sensor to avoid false positives from ambient noise. This was done by adjusting the potentiometer on the sound sensor module.
              </p>

              <h4>LEDs</h4>
              <p style="text-align: left;padding: 0px 30px;">
              The LEDs are used to provide visual feedback and enhance the user experience. We use a NeoPixel ring connected to the Raspberry Pi via a dedicated GPIO pin. The Adafruit NeoPixel library is used to control the LEDs, allowing us to set colors and animations. At startup, Binny performs a colorful LED animation to indicate that it is powering on. During operation, the LEDs can change colors based on Binny's state, such as: wiping a red across the strip if it is within range of a person to signify stopping, flashing a green light when it hears a clap to turn, flashing a blue light when it chases someone, wiping white across the strip when we transition from joystick to autonomous mode, and having a rainbow gradient fade on the strip when we are controlling the robot using a joystick. 
              The LED effects are managed in a separate python process from the main app since the NeoPixel library needs to be run as root, which the rest of the program was not configured to do (we ran into permission issues when running the ultrasonic code as root). Communication between the main app and the LED process is done via a simple command line protocol using the subprocess library. We can run a command such as "sudo -E python light_test.py --mode wipe --color 0,255,0" from our main Python program to spawn a process that is run as root to handle the LEDs. This allows us to change LED effects in real-time based on Binny's state without blocking the main program flow.
              A problem we encountered with the LEDs is that if the LED process is not properly terminated, the pins controlling the LEDs can get stuck in their last state, and conflict with any subsequent calls, causing the whole system to lag and the LEDs to flash erratically or just not change color. To solve this, we implemented a cleanup routine that is called when the main program exits, which sends a command to the LED process to properly de-initialize the pin used to control the LEDs. Additionally, we made sure to prevent multiple LED processes from running simultaneously by checking for existing processes before spawning a new one, and by limiting the time between LED processes to at least 5 seconds.
              The large gap between LED commands is to prevent multiple LED processes from running at once, which would cause the system to lag and the LEDs to behave erratically.
              Additionally, to run the LED process at startup with a set animation, we have a bash script to run a separate program that handles the LED startup animation, that then runs the main Binny program after the animation is complete.
              A nother large problem we had with these LEDs is that they draw a relatively a lot of voltage, which when powered directly from the Pi led to issues with controlling the camera, screen, and motors all at once which already is taxing. To solve this, we powered the LEDs from a separate 5V power supply via the motor driver to offload the power draw from the Pi.

              <h4>Website and Hotspot</h4>
              <p style="text-align: left;padding: 0px 30px;">
              The website serves as a user interface for controlling Binny. It is hosted on the Raspberry Pi using the Flask web framework. The website allows users to toggle between manual joystick control and autonomous mode. In joystick mode, users can control Binny's movement using an on-screen joystick implemented with the nipplejs library. In autonomous mode, Binny uses its computer vision and obstacle avoidance algorithms to follow humans automatically. The website has a button to switch modes, which sends a request to the Flask server to update Binny's operating mode. The server then communicates this change to the main control loop of Binny, allowing it to switch between manual and autonomous operation.
              Additionally, The Raspberry Pi is configured to create a WiFi hotspot using networkmanager, allowing users to connect directly to Binny's network. This enables easy access to the website without needing an external WiFi network. The hotspot configuration includes setting up a static IP address for the Pi so users can access the website at a known address (10.42.0.1:5000).
              </p>

            
              <!-- <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p> -->
      </div>

    <hr id='testing'>

      <div style="text-align:center;">
              <h2>Testing</h2>
              motor speed differences (not going straight),
              motor wiring issues
              motor driver issues
              improving how to turn towards person
              improving confidence score threshold
              Pitft and espeak process laggings
              led not cleaning up when finished


              <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p>
      </div>

    <hr id='result'>

      <div style="text-align:center;">
              <h2>Result</h2>
              <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p>
      </div>

    <hr>

    <div class="row" style="text-align:center;">
          <h2>Work Distribution</h2>
          <div style="text-align:center;">
              <img class="img-rounded" src="pics/group.jpg" alt="Generic placeholder image" style="width:80%;">
              <h4>Project group picture</h4>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/a.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Rick</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Designed the overall software architecture (Just being himself).
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/b.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Morty</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Tested the overall system.
          </div>
      </div>

    <hr>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li>Raspberry Pi $35.00</li>
              <li>Raspberry Pi Camera V2 $25.00</li>
              <a href="https://www.adafruit.com/product/1463"><li>NeoPixel Ring - $9.95</li></a>
              <li>LEDs, Resistors and Wires - Provided in lab</li>
          </ul>
          <h3>Total: $69.95</h3>
      </div>
      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
          <a href="http://getbootstrap.com/">Bootstrap</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>

      </div>

    <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>
